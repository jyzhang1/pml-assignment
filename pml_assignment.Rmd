---
title       : Practical Machine Learning Assignment
subtitle    : Coursera Data Science Specialization
author      : Jinyan Zhang
output: 
        html_document:
                fig_caption: yes
---
#### Introduction
The goal of the assignment is fit a prediction model to determine the type of exercise performed using data obtained from six participants. 

Briefly, based on the data obtained from various accelerometers attached onto their bodies and the dumbbell used in the exercise, we are supposed to determine the type of exercises (Class A, B, C, D, or E) performed by the participants. The data are splitted into a training and a testing set. 

More information can be obtained from [Weight Lefting Exercises Dataset](http://groupware.les.inf.puc-rio.br/har#weight_lifting_exercises).


#### Procedures
```{r "setup", cache = FALSE, echo = FALSE, message = FALSE, warning = FALSE, tidy = FALSE}
# make this an external chunk that can be included in any file
options(width = 100)
library(knitr)
opts_chunk$set(fig.align = 'center', echo = FALSE, fig.height = 4, 
               fig.width = 4, cache = TRUE, warning = FALSE)
```

Load the necessary libraries and dataset into **RStudio**.

``` {r "loading dataset", echo = TRUE, results = "hide"}
library(caret)
library(randomForest)
library(gbm)
library(survival)
library(splines)
library(parallel)
library(plyr)
library(MASS)
train <- read.csv("pml-training.csv", na.strings = c("", "NA"))
```

Next, we look at the data and perform some simple exploratory data analysis. This step has to be done so that you know what information are carried in the dataset and choose the variables needed to fit the prediction model.

``` {r "exploratory data analysis"}
names(train)
str(train)
```

You'll notice some columns do not carry any data or incomplete data. We will figure out which columns are those as we need to remove those columns from the model. Also, certain columns for e.g. `kurtosis_roll_arm` or `max_yaw_belt` are classified as `Factor` but the data are supposedly `numeric` in nature. In addition, some values are missing as there are a number of `NA` at the back of the `str` output. We will remove those columns in the initial stage of analysis and check if the prediction model works, if not, we can always look back at these data. 

``` {r "cleaning data", echo = TRUE}
# first, create a function to check for NA values
checkNA <- function(vector) {
        sum(!is.na(vector))
}

values <- sapply(train, checkNA)
values
# the 'values' object output will show that there are two categories of data,
# those with 19622 rows of data and those with 406 rows
# we will use those columns with 19622 rows of data since they are complete
col.full <- which(unname(values) > 10000)

length(col.full)
# 100 columns of data are removed in the process

# then, subset the required columns carrying 19622 rows of data
trainS <- subset(train, select = col.full)

# lastly, remove the columns that are not readouts from the accelerometers
# the time is also removed as logically they should not be predictors
trainS <- trainS[, -(1:7)]

length(names(trainS))
# so, we are left with 53 - 1 predictors to build the predictor model
# 53 - 1 because the last column is the column carrying the response
```

An important part about machine learning is choosing which predictors (or independent variables) include into the model fitting and which predictors to exclude. This might take some trial and error with the train and the test sets from the training dataset.

Utimately, the out-of-sample error when the model is fitted onto the test cannot be eliminated. Thus the in-sample error is minimized as much as possible during the training phase. 

We will then go on to split our data from `pml-training.csv` via conventional validation, which is to split the data set into a training and testing set. We can proceed on without cross validation first as the data set is huge (19622 rows of observations). If the prediction model does not have a high accuracy, we can always come back here and perform various cross validation methods such as **random subsampling** or **K-fold cross validation**.
``` {r "training & testing", echo = TRUE}
# set seed to ensure repeatable results
set.seed(12345)
inTrain <- createDataPartition(y = trainS$classe, p = 0.7, list = FALSE)
training <- trainS[inTrain, ] 
testing <- trainS[-inTrain, ]

# check the dimension of the training and testing set
dim(training)
dim(testing)
```

Three prediction methods: random forest, boosting and linear discriminant analysis, will be used. The three methods will then be stacked and ran to check which model has the best accuracy. The one with the highest accuracy will be used to determine the exercise performed in the `pml-testing.csv` dataset. 

``` {r "model fitting rf", results = "hide"}
fit.rf <- train(classe ~ ., data = training, method = "rf")
fit.gbm <- train(classe ~ ., data = training, method = "gbm")
fit.lda <- train(classe ~ ., data = training, method = "lda")
```


``` {r "prediction", echo = TRUE}
# individual models: random forest, boosting, linear discriminant analysis
pred.rf <- predict(fit.rf, newdata = testing)
pred.gbm <- predict(fit.gbm, newdata = testing)
pred.lda <- predict(fit.lda, newdata = testing)

# employing ensemble learning to see if it works better than individual models
predDf <- data.frame(pred.rf, pred.gbm, pred.lda, classe = testing$classe)
stack.rf <- train(classe ~ ., data = predDf, method = "rf")
pred.stack <- predict(stack.rf, newdata = predDf)

# checking the accuracies of the four prediction models
heading <- c("Random Forests", "Boosting", "Linear Discriminant Analysis", "Stacked Models")
rownames <- "Accuracy"
matrixName <- list(rownames, heading)
matrix(c(
        round(confusionMatrix(pred.rf, testing$classe)$overall[1], 3),
        round(confusionMatrix(pred.gbm, testing$classe)$overall[1], 3),
        round(confusionMatrix(pred.lda, testing$classe)$overall[1], 3),
        round(confusionMatrix(pred.stack, testing$classe)$overall[1], 3)
        ), 
        ncol = 4,
        dimnames = matrixName)

```

Lastly, the prediction model, random forests,  is used to predict the exercises performed in the `pml-testing.csv` dataset, since it has the highest accuracy and the same accuracy as the stacked model. Using the stacked model will increase computation time without added accuracy to the prediction. The responses are the answer for the subsequent **Course Project Prediction Quiz**.

``` {r "prediction 2"}
test <- read.csv("pml-testing.csv", na.strings = c("", "NA"))
pred2 <- predict(fit.rf, newdata = test)
# B A B A A E D B A A B C B A E E A B B B
```

The responses to the quiz are ``r pred2``. 